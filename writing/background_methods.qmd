---
title: "Background and Methods"
format: html
editor: visual
---

## Structure

-   zero inflation -\> bayesian vs frequentist -\> FIA data -\> ...

### Data Structure

#### Zero-Inflation

First some housekeeping. As the name suggests, data are canonically classified as being zero-inflated when they contain a significant proportion of zeroes. While it's hardly ever very productive to spell out a definition for a phrase that is it's own definition, I do so here to emphasize the fact that to call data zero-inflated is to only say something very broad about how that data is distributed. There is no commonly accepted cutoff for at what proportion of zeros our data deserves the label zero-inflation, and there is no restriction on the distribution of the non-zero data. While the work done in this thesis concerns zero-inflated data with no constraint on the level of "zeroness", I do require that the non-zero data is positive and continuously distributed. Importantly, as I am working in a modeling setting, **when I say our data is zero-inflated I mean that our response variable is zero-inflated**. For example, the response variable might look like this:

```{r, echo=F, message=F, warning=F}
set.seed(103)
library(tidyverse)

z <- tibble(
  z = rep(0, 2500)
)

nz <- tibble(
  nz = rgamma(1000, 3, 4)
)

ggplot() +
  geom_histogram(
    data = z,
    aes(x = z),
    fill = "midnightblue",
    color = "white",
    alpha = 0.8
    ) +
  geom_density(
    data = nz,
    aes(x = nz, y = ..count..),
    fill = "midnightblue",
    color = "white",
    alpha = 0.8
    ) +
  labs(
    x = "Response Variable"
  ) +
  theme_bw()
```

Although for the duration of this thesis I will simply refer to our data as being zero-inflated and our models as being suited for zero-inflated data, this is simply a matter of convenience and not a statement that the methods work for any situation in which data can be considered zero-inflated.

#### Modeling struggles

To motivate these methods, I'll first show what happens when I try to just fit a simple linear regression to this type of data.

```{r}
DGP_s1 <- function(n) {
  
  X <- rnorm(n, 0, 1) 
  beta_z <- 1
  xb <- X*beta_z 
  prob <- exp(xb) / (1 + exp(xb)) 
  Z <- 1*(runif(n)<prob)
  beta_y <- 1.5
  ypure <- 5 + X*beta_y 
  Y <- rgamma(n, shape = 4, rate = 1/ypure) 
  Y <- Z*Y 

  return(tibble(X, Y, Z))
  
}

data <- DGP_s1(1000)

data %>% 
  ggplot(aes(x = X, y = Y)) + 
  geom_point(size = 3, color = "grey20", alpha = 0.6) +
  stat_smooth(
    geom = "line", method = "lm",
    se = F, alpha = 0.75,
    linewidth = 2.5, color = "#BB2649") +
  theme_bw()
```


While this model isn't awful it's certainly missspecified. The zero-inflation in our response variable pulls the regression line down so that it doesn't properly capture the relationship between our explanatory variable and our *non-zero*  response, but more importantly it doesn't capture the structure of zeros in our response at all. Additionally, this model sometimes predicts negative values for our response variable which is positive (usually predictions which are impossible are not a good thing).

We would call this model statistically biased, as it is overly simple and thus doesn't properly capture the structure of the data. As one might imagine the bias of this model will be progressively worse as the level of zero-inflation increases.

### FIA

The United States Forestry Inventory and Analysis Program (FIA) monitors the nation's forests by collecting data on, and providing estimates for, a wide array of forest attributes. Not only is this work vitally important, but it's essential that it be done accurately and efficiently: "\[The\] FIA is responsible for reporting on dozens, if not hundreds, of forest attributes relating to merchantable timber and other wood products, fuels and potential fire hazard, condition of wildlife habitats, risk associated with fire, insects or disease, biomass, carbon storage, forest health, and other general characteristics of forest ecosystems." (McConville 2020).

To assess forest metrics across the United States, the FIA employs a quasi-systematic sampling design to collect data at ground plots across the U.S. The FIA employs a stratified sampling approach to selecting these ground plots, first partitioning the entire U.S. into 6000 acre hexagons and then randomly sampling locations from within these hexagons for measurement.

(add image here)

These sampled locations are referred to as plot-level data and the FIA sends a crew out to physically measure a wealth of forest attributes at that location. As you might expect, not only is this method extremely time intensive, but it is also very expensive. The vastness of the nation's forests in tandem with the resources needed to collect plot-level data, make it impossible to collect census level data on forest metrics. Thus, the need for additional data sources as well as statistical models are vital to the work that the FIA does. The main secondary data source that the FIA employes is remote sensed data. The remote sensed data typically includes climate metrics (e.g. temperature and precipitation), geomorphological measures (e.g. elevation and eastness), as well as metrics like tree canopy cover which can be measured from a satellite. These remote sensed data are referred to as pixel-level data and can be hugely helpful in reducing the variability in the FIA's estimates.

Importantly, these estimates are typically generated for certain areas of interest. For example, at one extreme, the FIA might want to know what the average tree cover is in the state of California. In a setting such as this one, it will be fairly easy to produce accurate estimates because our area of interest is spatially large, in small areas of interest, such as counties, it is much harder.
